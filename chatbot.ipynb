{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "032b8150",
   "metadata": {},
   "source": [
    "# Building a Simple Chatbot in Python using NLTK\n",
    "\n",
    "The main technology which helps with the functioning of the chatbot is NLP(Natural Language Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4322a7d",
   "metadata": {},
   "source": [
    "# NLP\n",
    "\n",
    "NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way. By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef6b71e",
   "metadata": {},
   "source": [
    "# Importing the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97f93929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018bcafd",
   "metadata": {},
   "source": [
    "# Downloading and Installing NLTK\n",
    "\n",
    "NLTK(Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bf13164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ribhq\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\ribhq\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ribhq\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2022.7.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ribhq\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\users\\ribhq\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\ribhq\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed4c22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ribhq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ribhq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('popular', quiet=True)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98a63ad",
   "metadata": {},
   "source": [
    "# Corpus\n",
    "\n",
    "I used Wikipedia pages for chatbots as my corpus. Copy the contents from the page and place it in a text file named ‘chatbot.txt’. However, you can use any corpus of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "043edd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('chatbot.txt','r',errors = 'ignore')\n",
    "raw = f.read()\n",
    "raw = raw.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9250dd93",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12ea9625",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw)\n",
    "word_tokens = nltk.word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83757ad8",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "\n",
    "We shall now define a function called LemTokens which will take as input the tokens and return normalized tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77273be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fead9fd",
   "metadata": {},
   "source": [
    "# Keyword Matching\n",
    "\n",
    "We shall define a function for a greeting by the bot i.e if a user’s input is a greeting, the bot shall return a response. ELIZA uses a simple keyword matching for greetings. We will utilize the same concept here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0cacb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",\"yo\")\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d146c6",
   "metadata": {},
   "source": [
    "# Generating a Response\n",
    "\n",
    "We define a function response that searches the user’s input for one or more programmed keywords and returns one of several possible responses. If it doesn’t find the input matching any of the keywords, it returns a response: “I don’t understand you”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7178274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    spar_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        spar_response=spar_response+\"I don’t understand you\"\n",
    "        return spar_response\n",
    "    else:\n",
    "        spar_response = spar_response+sent_tokens[idx]\n",
    "        return spar_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae944a0",
   "metadata": {},
   "source": [
    "Now, we will command statements that we want the Bot to say while starting and ending a conversation upon the user’s input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae385969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spar: My name is Spar. I will answer your queries about Chatbots. If you want to exit, type Bye!\n",
      "hello\n",
      "Spar: hello\n",
      "bye\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"Spar: My name is Spar. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"Spar: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"Spar: \" + greeting(user_response))\n",
    "            else:\n",
    "                print(\"Spar: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e308fa17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
